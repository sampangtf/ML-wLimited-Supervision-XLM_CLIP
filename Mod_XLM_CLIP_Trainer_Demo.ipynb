{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Mounting"
      ],
      "metadata": {
        "id": "DtIWaZT08HPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers==4.22.2\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "kxGuk79s7m3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adbf7db3-d9c1-48b0-f290-c46ddb51e2ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 7.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "# from transformers import CLIPProcessor, CLIPModel\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "SPFUMLa3Bg6k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "git_token = \"Your Persoanl Access Token in Georgia Tech Github\""
      ],
      "metadata": {
        "id": "byfTaA4hKTWG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{git_token}@github.gatech.edu/tpang34/XLM_CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjba1ZtGKLNQ",
        "outputId": "7d42a0c4-776f-439f-e8e3-b05cae44d0fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'XLM_CLIP'...\n",
            "remote: Enumerating objects: 3117, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 3117 (delta 20), reused 31 (delta 13), pack-reused 3072\u001b[K\n",
            "Receiving objects: 100% (3117/3117), 11.26 MiB | 5.20 MiB/s, done.\n",
            "Resolving deltas: 100% (824/824), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/XLM_CLIP/transformers_mod/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCnL6QBw3ZIJ",
        "outputId": "a12b756a-1ed5-48a5-afee-3a9b4b0f7eb9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./XLM_CLIP/transformers_mod\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.25.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.25.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (2.10)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.25.0.dev0-py3-none-any.whl size=5711750 sha256=379c5d66bfed4e8cbde6845f5feb3bb996c23a9789cc1dacc17f6f88e5100c06\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_banuzqb/wheels/fb/24/73/8ceb98fcf59b49e5a97a4a4dc7b95e2770d6ac16b0246ced3c\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.25.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPModel"
      ],
      "metadata": {
        "id": "dVOLP9dx4AV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "_CL-AMeI4Amr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "device = 'cuda'\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")# .to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9ujEBCt7r-7",
        "outputId": "bf584255-fab1-4826-d76e-e5a912e94d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CLIPModel were not initialized from the model checkpoint at openai/clip-vit-base-patch32 and are newly initialized: ['xlm_encoder.encoder.layer.22.attention.self.query.weight', 'xlm_encoder.encoder.layer.20.output.dense.bias', 'xlm_encoder.encoder.layer.23.attention.self.value.bias', 'xlm_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.6.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.8.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.1.output.dense.weight', 'xlm_encoder.encoder.layer.15.output.dense.bias', 'xlm_encoder.encoder.layer.17.attention.self.query.bias', 'xlm_encoder.encoder.layer.14.attention.self.key.bias', 'xlm_encoder.encoder.layer.11.attention.self.key.weight', 'xlm_encoder.encoder.layer.20.attention.self.key.bias', 'xlm_encoder.encoder.layer.0.attention.self.value.bias', 'xlm_encoder.encoder.layer.0.output.dense.weight', 'xlm_encoder.encoder.layer.6.output.dense.weight', 'xlm_encoder.encoder.layer.14.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.18.attention.self.key.weight', 'xlm_encoder.encoder.layer.2.attention.self.value.bias', 'xlm_encoder.encoder.layer.8.intermediate.dense.bias', 'xlm_encoder.encoder.layer.1.attention.output.dense.weight', 'xlm_encoder.encoder.layer.21.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.1.attention.self.value.weight', 'xlm_encoder.encoder.layer.20.attention.output.LayerNorm.bias', 'xlm_encoder.embeddings.LayerNorm.bias', 'xlm_encoder.encoder.layer.5.attention.self.key.bias', 'xlm_encoder.encoder.layer.13.output.dense.bias', 'xlm_encoder.encoder.layer.23.attention.self.key.bias', 'xlm_encoder.encoder.layer.11.intermediate.dense.bias', 'xlm_encoder.encoder.layer.18.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.7.intermediate.dense.weight', 'xlm_encoder.encoder.layer.13.attention.self.value.weight', 'xlm_encoder.encoder.layer.18.output.dense.weight', 'xlm_encoder.encoder.layer.22.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.13.attention.self.query.bias', 'xlm_encoder.encoder.layer.5.output.dense.bias', 'xlm_encoder.encoder.layer.6.intermediate.dense.weight', 'xlm_encoder.encoder.layer.10.output.dense.bias', 'xlm_encoder.encoder.layer.19.attention.self.query.weight', 'xlm_encoder.encoder.layer.3.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.17.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.19.attention.output.dense.bias', 'xlm_encoder.encoder.layer.11.attention.output.dense.bias', 'xlm_encoder.encoder.layer.16.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.8.attention.self.key.bias', 'xlm_encoder.encoder.layer.19.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.19.output.dense.bias', 'xlm_encoder.encoder.layer.13.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.13.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.9.attention.output.dense.bias', 'xlm_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.19.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.21.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.23.attention.self.query.weight', 'xlm_encoder.encoder.layer.22.attention.self.value.bias', 'xlm_encoder.encoder.layer.0.intermediate.dense.bias', 'xlm_encoder.encoder.layer.0.attention.self.query.weight', 'xlm_encoder.encoder.layer.7.attention.self.query.weight', 'xlm_encoder.encoder.layer.5.attention.self.value.bias', 'xlm_encoder.encoder.layer.3.attention.self.query.weight', 'xlm_encoder.encoder.layer.16.attention.self.key.weight', 'xlm_encoder.encoder.layer.6.attention.self.key.weight', 'xlm_encoder.encoder.layer.9.attention.self.query.bias', 'xlm_encoder.encoder.layer.7.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.20.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.1.attention.self.key.bias', 'xlm_encoder.encoder.layer.19.attention.self.key.weight', 'xlm_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.7.attention.self.value.bias', 'xlm_encoder.encoder.layer.20.attention.self.query.bias', 'xlm_encoder.encoder.layer.10.attention.output.dense.weight', 'xlm_encoder.encoder.layer.12.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.9.intermediate.dense.weight', 'xlm_encoder.encoder.layer.0.intermediate.dense.weight', 'xlm_encoder.encoder.layer.3.attention.self.key.bias', 'xlm_encoder.encoder.layer.9.attention.self.key.bias', 'xlm_encoder.encoder.layer.14.attention.self.key.weight', 'xlm_encoder.encoder.layer.18.output.LayerNorm.bias', 'xlm_encoder.pooler.dense.bias', 'xlm_encoder.encoder.layer.10.intermediate.dense.bias', 'xlm_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.4.attention.output.dense.bias', 'xlm_encoder.encoder.layer.19.attention.self.query.bias', 'xlm_encoder.encoder.layer.2.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.20.attention.self.query.weight', 'xlm_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.20.output.dense.weight', 'xlm_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.18.attention.output.dense.weight', 'xlm_encoder.encoder.layer.14.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.2.attention.self.query.bias', 'xlm_encoder.encoder.layer.20.intermediate.dense.bias', 'xlm_encoder.encoder.layer.6.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.21.attention.self.query.bias', 'xlm_encoder.encoder.layer.14.attention.self.value.bias', 'xlm_encoder.encoder.layer.7.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.19.intermediate.dense.weight', 'xlm_encoder.encoder.layer.4.intermediate.dense.weight', 'xlm_encoder.encoder.layer.21.attention.self.query.weight', 'xlm_encoder.encoder.layer.19.output.dense.weight', 'xlm_encoder.encoder.layer.14.attention.self.value.weight', 'xlm_encoder.encoder.layer.12.attention.self.key.weight', 'xlm_encoder.encoder.layer.11.output.dense.bias', 'xlm_encoder.encoder.layer.13.attention.output.dense.bias', 'xlm_encoder.encoder.layer.18.output.dense.bias', 'xlm_encoder.encoder.layer.11.attention.self.value.bias', 'xlm_encoder.encoder.layer.8.attention.self.key.weight', 'xlm_encoder.encoder.layer.21.attention.output.dense.bias', 'xlm_encoder.encoder.layer.6.attention.output.dense.bias', 'xlm_encoder.encoder.layer.9.output.dense.bias', 'xlm_encoder.encoder.layer.6.attention.self.value.weight', 'xlm_encoder.encoder.layer.22.output.dense.bias', 'xlm_encoder.encoder.layer.19.attention.self.key.bias', 'xlm_encoder.encoder.layer.16.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.20.attention.self.value.weight', 'xlm_encoder.encoder.layer.7.attention.output.dense.weight', 'xlm_encoder.encoder.layer.6.output.dense.bias', 'xlm_encoder.encoder.layer.12.output.dense.weight', 'xlm_encoder.encoder.layer.16.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.13.intermediate.dense.bias', 'xlm_encoder.encoder.layer.20.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.22.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.7.attention.self.query.bias', 'xlm_encoder.encoder.layer.17.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.5.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.1.attention.self.value.bias', 'xlm_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.8.attention.self.query.weight', 'xlm_encoder.encoder.layer.18.intermediate.dense.bias', 'xlm_encoder.encoder.layer.8.attention.self.value.bias', 'xlm_encoder.encoder.layer.18.attention.output.dense.bias', 'xlm_encoder.encoder.layer.19.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.1.attention.self.query.bias', 'xlm_encoder.encoder.layer.4.attention.self.query.bias', 'xlm_encoder.encoder.layer.9.attention.self.value.bias', 'xlm_encoder.encoder.layer.21.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.12.attention.self.value.bias', 'xlm_encoder.encoder.layer.12.intermediate.dense.weight', 'xlm_encoder.encoder.layer.18.intermediate.dense.weight', 'xlm_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.16.attention.self.query.bias', 'xlm_encoder.encoder.layer.21.output.dense.bias', 'xlm_encoder.encoder.layer.8.output.dense.bias', 'xlm_encoder.encoder.layer.14.attention.self.query.bias', 'xlm_encoder.encoder.layer.14.intermediate.dense.weight', 'xlm_encoder.encoder.layer.18.attention.self.value.bias', 'xlm_encoder.encoder.layer.21.attention.self.key.weight', 'xlm_encoder.encoder.layer.22.intermediate.dense.bias', 'xlm_encoder.encoder.layer.17.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.9.attention.self.query.weight', 'xlm_encoder.encoder.layer.21.attention.output.dense.weight', 'xlm_encoder.encoder.layer.19.attention.self.value.bias', 'xlm_encoder.encoder.layer.16.output.dense.weight', 'xlm_encoder.encoder.layer.11.attention.self.query.bias', 'xlm_encoder.encoder.layer.17.attention.output.dense.bias', 'xlm_encoder.encoder.layer.0.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.15.intermediate.dense.bias', 'xlm_encoder.encoder.layer.5.attention.self.value.weight', 'xlm_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.21.intermediate.dense.weight', 'xlm_encoder.encoder.layer.9.attention.output.dense.weight', 'xlm_encoder.encoder.layer.12.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.13.attention.self.query.weight', 'agg_text_projection.weight', 'xlm_encoder.encoder.layer.16.attention.self.value.bias', 'xlm_encoder.encoder.layer.14.attention.output.dense.weight', 'xlm_encoder.encoder.layer.18.attention.self.key.bias', 'xlm_encoder.encoder.layer.17.attention.self.key.weight', 'xlm_encoder.encoder.layer.23.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.17.output.dense.weight', 'xlm_encoder.encoder.layer.17.output.dense.bias', 'xlm_encoder.encoder.layer.15.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.9.output.dense.weight', 'xlm_encoder.encoder.layer.7.attention.self.key.bias', 'xlm_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.4.attention.self.query.weight', 'xlm_encoder.encoder.layer.4.attention.self.value.bias', 'xlm_encoder.encoder.layer.14.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.3.attention.self.value.bias', 'xlm_encoder.encoder.layer.18.attention.self.query.bias', 'xlm_encoder.encoder.layer.5.attention.self.query.bias', 'xlm_encoder.encoder.layer.23.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.17.attention.self.query.weight', 'xlm_encoder.encoder.layer.23.attention.self.value.weight', 'xlm_encoder.encoder.layer.2.attention.output.dense.weight', 'xlm_encoder.encoder.layer.0.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.4.attention.output.dense.weight', 'xlm_encoder.encoder.layer.23.output.dense.bias', 'xlm_encoder.encoder.layer.14.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.13.attention.output.dense.weight', 'xlm_encoder.encoder.layer.10.output.dense.weight', 'xlm_encoder.encoder.layer.20.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.4.intermediate.dense.bias', 'xlm_encoder.encoder.layer.10.attention.self.key.weight', 'xlm_encoder.encoder.layer.15.attention.self.key.weight', 'xlm_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.23.attention.self.key.weight', 'xlm_encoder.encoder.layer.18.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.9.intermediate.dense.bias', 'xlm_encoder.encoder.layer.15.attention.self.query.weight', 'xlm_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.16.attention.self.query.weight', 'xlm_encoder.encoder.layer.13.attention.self.value.bias', 'xlm_encoder.encoder.layer.11.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.22.attention.self.value.weight', 'xlm_encoder.encoder.layer.1.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.12.attention.self.query.weight', 'xlm_encoder.encoder.layer.6.intermediate.dense.bias', 'xlm_encoder.encoder.layer.16.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.12.attention.output.dense.bias', 'xlm_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.22.output.dense.weight', 'xlm_encoder.encoder.layer.17.attention.self.key.bias', 'xlm_encoder.encoder.layer.2.attention.self.key.weight', 'xlm_encoder.encoder.layer.4.attention.self.key.weight', 'xlm_encoder.encoder.layer.12.attention.self.query.bias', 'xlm_encoder.encoder.layer.14.intermediate.dense.bias', 'xlm_encoder.encoder.layer.21.output.dense.weight', 'xlm_encoder.encoder.layer.0.attention.self.key.weight', 'xlm_encoder.encoder.layer.22.intermediate.dense.weight', 'xlm_encoder.encoder.layer.10.attention.output.dense.bias', 'xlm_encoder.encoder.layer.22.attention.self.query.bias', 'xlm_encoder.encoder.layer.1.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.15.output.dense.weight', 'xlm_encoder.encoder.layer.4.attention.self.value.weight', 'xlm_encoder.encoder.layer.1.attention.self.key.weight', 'xlm_encoder.encoder.layer.4.output.dense.bias', 'xlm_encoder.encoder.layer.5.output.dense.weight', 'xlm_encoder.encoder.layer.17.intermediate.dense.bias', 'xlm_encoder.encoder.layer.0.attention.output.dense.bias', 'xlm_encoder.encoder.layer.15.attention.self.query.bias', 'xlm_encoder.encoder.layer.1.intermediate.dense.weight', 'xlm_encoder.encoder.layer.11.attention.self.key.bias', 'xlm_encoder.encoder.layer.11.attention.self.value.weight', 'xlm_encoder.encoder.layer.14.attention.self.query.weight', 'xlm_encoder.embeddings.token_type_embeddings.weight', 'xlm_encoder.encoder.layer.15.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.2.output.dense.bias', 'xlm_encoder.encoder.layer.8.attention.self.query.bias', 'xlm_encoder.encoder.layer.7.intermediate.dense.bias', 'xlm_encoder.encoder.layer.12.intermediate.dense.bias', 'xlm_encoder.encoder.layer.9.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.14.output.dense.bias', 'xlm_encoder.encoder.layer.4.output.LayerNorm.weight', 'xlm_encoder.pooler.dense.weight', 'xlm_encoder.encoder.layer.10.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.8.attention.output.dense.bias', 'xlm_encoder.encoder.layer.10.attention.self.value.weight', 'xlm_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.3.output.dense.bias', 'xlm_encoder.encoder.layer.12.output.dense.bias', 'xlm_encoder.encoder.layer.15.intermediate.dense.weight', 'xlm_encoder.encoder.layer.2.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.4.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.16.intermediate.dense.weight', 'xlm_encoder.encoder.layer.1.intermediate.dense.bias', 'xlm_encoder.encoder.layer.2.attention.self.key.bias', 'xlm_encoder.encoder.layer.2.attention.self.value.weight', 'xlm_encoder.encoder.layer.8.output.dense.weight', 'xlm_encoder.encoder.layer.4.output.dense.weight', 'xlm_encoder.encoder.layer.23.intermediate.dense.weight', 'xlm_encoder.encoder.layer.5.attention.self.query.weight', 'xlm_encoder.encoder.layer.22.attention.self.key.weight', 'xlm_encoder.encoder.layer.3.attention.self.key.weight', 'xlm_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.5.intermediate.dense.weight', 'xlm_encoder.encoder.layer.11.attention.output.dense.weight', 'xlm_encoder.encoder.layer.0.attention.output.dense.weight', 'xlm_encoder.encoder.layer.7.output.dense.bias', 'xlm_encoder.encoder.layer.12.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.12.attention.self.key.bias', 'xlm_encoder.encoder.layer.12.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.6.attention.self.value.bias', 'xlm_encoder.encoder.layer.17.attention.output.dense.weight', 'xlm_encoder.encoder.layer.20.intermediate.dense.weight', 'xlm_encoder.encoder.layer.10.attention.self.query.weight', 'xlm_encoder.encoder.layer.6.attention.self.key.bias', 'xlm_encoder.encoder.layer.15.attention.self.value.weight', 'xlm_encoder.encoder.layer.10.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.16.attention.output.dense.bias', 'xlm_encoder.encoder.layer.5.attention.output.dense.weight', 'xlm_encoder.encoder.layer.3.output.dense.weight', 'xlm_encoder.encoder.layer.21.intermediate.dense.bias', 'xlm_encoder.embeddings.LayerNorm.weight', 'xlm_encoder.encoder.layer.15.attention.output.dense.bias', 'xlm_encoder.encoder.layer.15.attention.output.dense.weight', 'xlm_encoder.encoder.layer.21.attention.self.value.bias', 'xlm_encoder.encoder.layer.3.attention.self.value.weight', 'xlm_encoder.encoder.layer.16.output.dense.bias', 'xlm_encoder.encoder.layer.19.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.20.attention.self.value.bias', 'xlm_encoder.encoder.layer.22.attention.output.dense.weight', 'xlm_encoder.encoder.layer.16.intermediate.dense.bias', 'xlm_encoder.encoder.layer.19.attention.output.dense.weight', 'xlm_encoder.encoder.layer.23.attention.self.query.bias', 'xlm_encoder.encoder.layer.3.attention.output.dense.bias', 'xlm_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.22.attention.self.key.bias', 'xlm_encoder.encoder.layer.5.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.1.attention.self.query.weight', 'xlm_encoder.encoder.layer.2.output.dense.weight', 'xlm_encoder.encoder.layer.10.attention.self.query.bias', 'xlm_encoder.encoder.layer.2.attention.self.query.weight', 'xlm_encoder.encoder.layer.23.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.20.attention.output.dense.weight', 'xlm_encoder.embeddings.position_embeddings.weight', 'xlm_encoder.encoder.layer.2.attention.output.dense.bias', 'xlm_encoder.encoder.layer.3.intermediate.dense.weight', 'xlm_encoder.encoder.layer.12.attention.self.value.weight', 'xlm_encoder.encoder.layer.7.output.dense.weight', 'xlm_encoder.encoder.layer.13.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.15.attention.self.value.bias', 'xlm_encoder.encoder.layer.7.attention.self.key.weight', 'xlm_encoder.encoder.layer.20.attention.self.key.weight', 'xlm_encoder.encoder.layer.11.attention.self.query.weight', 'xlm_encoder.encoder.layer.18.attention.self.query.weight', 'xlm_encoder.encoder.layer.10.intermediate.dense.weight', 'xlm_encoder.encoder.layer.9.attention.self.value.weight', 'xlm_encoder.encoder.layer.14.output.dense.weight', 'xlm_encoder.encoder.layer.1.attention.output.dense.bias', 'xlm_encoder.encoder.layer.8.intermediate.dense.weight', 'xlm_encoder.encoder.layer.13.attention.self.key.bias', 'xlm_encoder.encoder.layer.12.attention.output.dense.weight', 'xlm_encoder.encoder.layer.23.attention.output.dense.bias', 'xlm_encoder.encoder.layer.9.attention.self.key.weight', 'xlm_encoder.encoder.layer.8.attention.output.dense.weight', 'xlm_encoder.encoder.layer.17.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.23.output.dense.weight', 'xlm_encoder.encoder.layer.6.attention.self.query.weight', 'xlm_encoder.encoder.layer.3.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.13.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.5.attention.self.key.weight', 'xlm_encoder.encoder.layer.21.attention.self.value.weight', 'xlm_encoder.encoder.layer.19.attention.self.value.weight', 'xlm_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.22.attention.output.dense.bias', 'xlm_encoder.encoder.layer.16.attention.output.dense.weight', 'xlm_encoder.encoder.layer.22.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.9.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.20.attention.output.dense.bias', 'xlm_encoder.encoder.layer.15.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.13.output.dense.weight', 'xlm_encoder.encoder.layer.23.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.1.output.dense.bias', 'xlm_encoder.encoder.layer.16.attention.self.value.weight', 'xlm_encoder.encoder.layer.7.attention.output.dense.bias', 'xlm_encoder.encoder.layer.8.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.6.attention.self.query.bias', 'xlm_encoder.encoder.layer.2.intermediate.dense.bias', 'xlm_encoder.encoder.layer.6.attention.output.dense.weight', 'xlm_encoder.encoder.layer.21.attention.self.key.bias', 'xlm_encoder.encoder.layer.3.attention.self.query.bias', 'xlm_encoder.encoder.layer.18.attention.self.value.weight', 'xlm_encoder.encoder.layer.7.attention.self.value.weight', 'xlm_encoder.encoder.layer.17.attention.self.value.weight', 'xlm_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.3.intermediate.dense.bias', 'xlm_encoder.encoder.layer.13.intermediate.dense.weight', 'xlm_encoder.encoder.layer.19.intermediate.dense.bias', 'xlm_encoder.encoder.layer.11.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.0.attention.self.value.weight', 'xlm_encoder.encoder.layer.17.intermediate.dense.weight', 'xlm_encoder.encoder.layer.22.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.10.attention.self.value.bias', 'xlm_encoder.encoder.layer.5.attention.output.dense.bias', 'xlm_encoder.encoder.layer.23.attention.output.dense.weight', 'xlm_encoder.encoder.layer.10.attention.self.key.bias', 'xlm_encoder.embeddings.word_embeddings.weight', 'xlm_encoder.encoder.layer.13.attention.self.key.weight', 'xlm_encoder.encoder.layer.0.output.dense.bias', 'xlm_encoder.encoder.layer.15.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.18.attention.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.21.output.LayerNorm.weight', 'xlm_encoder.encoder.layer.8.attention.self.value.weight', 'xlm_encoder.encoder.layer.2.intermediate.dense.weight', 'xlm_encoder.encoder.layer.4.attention.self.key.bias', 'xlm_encoder.encoder.layer.0.attention.self.query.bias', 'xlm_encoder.encoder.layer.14.attention.output.dense.bias', 'xlm_encoder.encoder.layer.23.intermediate.dense.bias', 'xlm_encoder.encoder.layer.15.attention.self.key.bias', 'xlm_encoder.encoder.layer.3.attention.output.dense.weight', 'xlm_encoder.encoder.layer.16.attention.self.key.bias', 'xlm_encoder.encoder.layer.11.intermediate.dense.weight', 'xlm_encoder.encoder.layer.17.attention.self.value.bias', 'xlm_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'xlm_encoder.encoder.layer.5.intermediate.dense.bias', 'xlm_encoder.encoder.layer.0.attention.self.key.bias', 'xlm_encoder.encoder.layer.11.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image1 = Image.open(requests.get(url1, stream=True).raw)\n",
        "\n",
        "url2 = \"http://images.cocodataset.org/test-stuff2017/000000002383.jpg\"\n",
        "image2 = Image.open(requests.get(url2, stream=True).raw)\n",
        "\n",
        "text1 =  \"a photo of a cat\" # \"una foto de un gato\" \n",
        "text2 = \"a photo of two zebra\"\n",
        "\n",
        "xlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")# .to(device)\n",
        "xlm_inputs = xlm_tokenizer([text1], return_tensors=\"pt\").to(device)\n",
        "\n",
        "clip_inputs = clip_processor(\n",
        "    text=[text1], images=[image1, image2, image1], return_tensors=\"pt\", padding=True\n",
        ").to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "gi2U_cTm8V12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = clip_model(xlm_inputs=xlm_inputs, **clip_inputs)\n",
        "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=0)  # we can take the softmax to get the label probabilities\n"
      ],
      "metadata": {
        "id": "CiVqdz1r_L2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISjgWiUFKsYe",
        "outputId": "040ec4fd-8fb9-44e6-fcc5-298d451bab91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4990],\n",
              "        [0.0019],\n",
              "        [0.4990]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data"
      ],
      "metadata": {
        "id": "PFHRyZx68LiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download MS-COCO-Spanish\n",
        "!git clone https://github.com/carlosgarciahe/ms-coco-es"
      ],
      "metadata": {
        "id": "VQhp6RCg8U9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3dbb556-4e1d-432c-95ec-1ce170ce06c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'ms-coco-es' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "_FKATcn4GPag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# freezing all other layers\n",
        "for param in clip_model.parameters():\n",
        "    param.requires_grad = False\n",
        "clip_model.agg_text_projection.requires_grad_(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP1R5m_yCCNP",
        "outputId": "8e5484cf-31c1-408d-9fff-71f6a89a9790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=1536, out_features=512, bias=False)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_config = {\n",
        "  \"optimizer_type\": \"AdamW\",\n",
        "  \"lr\": 1e-2,\n",
        "  \"weight_decay\": 0,\n",
        "  \"momentum\": None}"
      ],
      "metadata": {
        "id": "NtTmkcqJGdyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rc4jWWRWZYVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = get_optimizer(clip_model, optimizer_config)"
      ],
      "metadata": {
        "id": "plWdsFYCXB-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/ms-coco-es\"\n",
        "model_base_path = 'clip/'"
      ],
      "metadata": {
        "id": "VlxoLt4GXB57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(data_dir= data_dir, \n",
        "                  model = clip_model,\n",
        "                  optimizer = optimizer,\n",
        "                  model_dir = model_base_path,\n",
        "                  load_from_disk = False,\n",
        "                  cuda = True\n",
        "                 )"
      ],
      "metadata": {
        "id": "gl5zDxtoW0v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(num_epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "MDXdFYb7X8fg",
        "outputId": "d914db46-9d8e-435b-92d2-351c2d2b7d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0, Training Loss:2.4402, Validation Loss:2.3874\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-b09a28f6b042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-87-4a9d21a607e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_img_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 14.76 GiB total capacity; 12.34 GiB already allocated; 361.75 MiB free; 13.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train_loss_list"
      ],
      "metadata": {
        "id": "WZaZKzL10FKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.plot_loss()\n",
        "trainer.plot_accuracy()\n",
        "trainer.plot_recall5()\n",
        "trainer.plot_recall10()"
      ],
      "metadata": {
        "id": "Potftfvbr1Ks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}